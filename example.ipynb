{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from mnist import load_data, train, evaluate, Net, HashedNet\n",
    "from utils import get_equivalent_compression\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(1)\n",
    "random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression rate: 0.015625\n",
      "The number of parameters is: 12730\n",
      "Epoch 1 Train loss: 0.698 Val loss: 0.298 Val acc: 91.05%\n",
      "Epoch 2 Train loss: 0.540 Val loss: 0.262 Val acc: 92.23%\n",
      "Epoch 3 Train loss: 0.526 Val loss: 0.263 Val acc: 91.97%\n",
      "Epoch 4 Train loss: 0.515 Val loss: 0.255 Val acc: 91.85%\n",
      "Epoch 5 Train loss: 0.505 Val loss: 0.243 Val acc: 92.58%\n",
      "Epoch 6 Train loss: 0.498 Val loss: 0.243 Val acc: 92.82%\n",
      "Epoch 7 Train loss: 0.501 Val loss: 0.243 Val acc: 92.80%\n",
      "Epoch 8 Train loss: 0.497 Val loss: 0.239 Val acc: 92.87%\n",
      "Epoch 9 Train loss: 0.493 Val loss: 0.239 Val acc: 92.70%\n",
      "Epoch 10 Train loss: 0.490 Val loss: 0.235 Val acc: 93.00%\n",
      "Epoch 11 Train loss: 0.493 Val loss: 0.237 Val acc: 93.20%\n",
      "Epoch 12 Train loss: 0.496 Val loss: 0.242 Val acc: 92.73%\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 13 Train loss: 0.479 Val loss: 0.238 Val acc: 92.75%\n",
      "Epoch 14 Train loss: 0.446 Val loss: 0.221 Val acc: 93.62%\n",
      "Epoch 15 Train loss: 0.435 Val loss: 0.220 Val acc: 93.50%\n",
      "Epoch 16 Train loss: 0.438 Val loss: 0.216 Val acc: 93.70%\n",
      "Epoch 17 Train loss: 0.438 Val loss: 0.216 Val acc: 93.62%\n",
      "Epoch 18 Train loss: 0.431 Val loss: 0.215 Val acc: 93.78%\n",
      "Epoch 19 Train loss: 0.428 Val loss: 0.216 Val acc: 93.73%\n",
      "Epoch 20 Train loss: 0.433 Val loss: 0.216 Val acc: 93.62%\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 21 Train loss: 0.430 Val loss: 0.216 Val acc: 93.78%\n",
      "Epoch 22 Train loss: 0.431 Val loss: 0.215 Val acc: 93.80%\n",
      "Epoch 23 Train loss: 0.422 Val loss: 0.214 Val acc: 93.70%\n",
      "Epoch 24 Train loss: 0.426 Val loss: 0.213 Val acc: 93.83%\n",
      "Epoch 25 Train loss: 0.428 Val loss: 0.213 Val acc: 93.78%\n",
      "Epoch 26 Train loss: 0.424 Val loss: 0.214 Val acc: 93.72%\n",
      "Epoch 27 Train loss: 0.427 Val loss: 0.214 Val acc: 93.77%\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 28 Train loss: 0.427 Val loss: 0.214 Val acc: 93.75%\n",
      "Epoch 29 Train loss: 0.424 Val loss: 0.213 Val acc: 93.75%\n",
      "Epoch 30 Train loss: 0.427 Val loss: 0.213 Val acc: 93.77%\n",
      "Epoch 31 Train loss: 0.427 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch 32 Train loss: 0.428 Val loss: 0.213 Val acc: 93.73%\n",
      "Epoch 33 Train loss: 0.427 Val loss: 0.213 Val acc: 93.73%\n",
      "Epoch 34 Train loss: 0.430 Val loss: 0.213 Val acc: 93.73%\n",
      "Epoch 35 Train loss: 0.427 Val loss: 0.213 Val acc: 93.73%\n",
      "Epoch 36 Train loss: 0.423 Val loss: 0.213 Val acc: 93.75%\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 37 Train loss: 0.426 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch 38 Train loss: 0.428 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch 39 Train loss: 0.422 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch 40 Train loss: 0.426 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch 41 Train loss: 0.423 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 42 Train loss: 0.423 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch 43 Train loss: 0.426 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch 44 Train loss: 0.426 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 45 Train loss: 0.425 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch 46 Train loss: 0.430 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch 47 Train loss: 0.425 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch 48 Train loss: 0.422 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch 49 Train loss: 0.424 Val loss: 0.213 Val acc: 93.72%\n",
      "Epoch 50 Train loss: 0.426 Val loss: 0.213 Val acc: 93.72%\n",
      "Test loss: 0.208 Test acc: 94.09%\n",
      "The number of parameters is: 12433\n",
      "Epoch 1 Train loss: 0.566 Val loss: 0.233 Val acc: 93.00%\n",
      "Epoch 2 Train loss: 0.496 Val loss: 0.223 Val acc: 93.68%\n",
      "Epoch 3 Train loss: 0.491 Val loss: 0.237 Val acc: 93.37%\n",
      "Epoch 4 Train loss: 0.505 Val loss: 0.226 Val acc: 93.83%\n",
      "Epoch     4: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 5 Train loss: 0.532 Val loss: 0.229 Val acc: 93.95%\n",
      "Epoch 6 Train loss: 0.302 Val loss: 0.153 Val acc: 95.38%\n",
      "Epoch 7 Train loss: 0.269 Val loss: 0.147 Val acc: 95.78%\n",
      "Epoch 8 Train loss: 0.260 Val loss: 0.143 Val acc: 95.70%\n",
      "Epoch 9 Train loss: 0.248 Val loss: 0.140 Val acc: 95.82%\n",
      "Epoch 10 Train loss: 0.244 Val loss: 0.134 Val acc: 95.98%\n",
      "Epoch 11 Train loss: 0.244 Val loss: 0.134 Val acc: 96.18%\n",
      "Epoch 12 Train loss: 0.235 Val loss: 0.132 Val acc: 96.00%\n",
      "Epoch 13 Train loss: 0.237 Val loss: 0.129 Val acc: 96.00%\n",
      "Epoch 14 Train loss: 0.229 Val loss: 0.127 Val acc: 96.18%\n",
      "Epoch 15 Train loss: 0.227 Val loss: 0.126 Val acc: 96.12%\n",
      "Epoch 16 Train loss: 0.227 Val loss: 0.126 Val acc: 96.20%\n",
      "Epoch 17 Train loss: 0.226 Val loss: 0.125 Val acc: 96.27%\n",
      "Epoch 18 Train loss: 0.225 Val loss: 0.121 Val acc: 96.35%\n",
      "Epoch 19 Train loss: 0.220 Val loss: 0.120 Val acc: 96.47%\n",
      "Epoch 20 Train loss: 0.219 Val loss: 0.120 Val acc: 96.48%\n",
      "Epoch 21 Train loss: 0.217 Val loss: 0.119 Val acc: 96.52%\n",
      "Epoch 22 Train loss: 0.214 Val loss: 0.118 Val acc: 96.40%\n",
      "Epoch 23 Train loss: 0.219 Val loss: 0.119 Val acc: 96.55%\n",
      "Epoch 24 Train loss: 0.213 Val loss: 0.117 Val acc: 96.43%\n",
      "Epoch 25 Train loss: 0.211 Val loss: 0.119 Val acc: 96.45%\n",
      "Epoch 26 Train loss: 0.211 Val loss: 0.115 Val acc: 96.48%\n",
      "Epoch 27 Train loss: 0.215 Val loss: 0.116 Val acc: 96.57%\n",
      "Epoch 28 Train loss: 0.209 Val loss: 0.114 Val acc: 96.63%\n",
      "Epoch 29 Train loss: 0.207 Val loss: 0.115 Val acc: 96.48%\n",
      "Epoch 30 Train loss: 0.204 Val loss: 0.116 Val acc: 96.47%\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 31 Train loss: 0.206 Val loss: 0.115 Val acc: 96.50%\n",
      "Epoch 32 Train loss: 0.201 Val loss: 0.112 Val acc: 96.65%\n",
      "Epoch 33 Train loss: 0.192 Val loss: 0.112 Val acc: 96.57%\n",
      "Epoch 34 Train loss: 0.194 Val loss: 0.111 Val acc: 96.58%\n",
      "Epoch 35 Train loss: 0.194 Val loss: 0.112 Val acc: 96.67%\n",
      "Epoch 36 Train loss: 0.195 Val loss: 0.112 Val acc: 96.77%\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 37 Train loss: 0.191 Val loss: 0.111 Val acc: 96.67%\n",
      "Epoch 38 Train loss: 0.197 Val loss: 0.111 Val acc: 96.60%\n",
      "Epoch 39 Train loss: 0.193 Val loss: 0.111 Val acc: 96.68%\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 40 Train loss: 0.192 Val loss: 0.111 Val acc: 96.65%\n",
      "Epoch 41 Train loss: 0.190 Val loss: 0.111 Val acc: 96.67%\n",
      "Epoch 42 Train loss: 0.195 Val loss: 0.111 Val acc: 96.67%\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 43 Train loss: 0.192 Val loss: 0.111 Val acc: 96.67%\n",
      "Epoch 44 Train loss: 0.195 Val loss: 0.111 Val acc: 96.67%\n",
      "Epoch 45 Train loss: 0.193 Val loss: 0.111 Val acc: 96.67%\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 46 Train loss: 0.193 Val loss: 0.111 Val acc: 96.67%\n",
      "Epoch 47 Train loss: 0.194 Val loss: 0.111 Val acc: 96.67%\n",
      "Epoch 48 Train loss: 0.192 Val loss: 0.111 Val acc: 96.67%\n",
      "Epoch 49 Train loss: 0.194 Val loss: 0.111 Val acc: 96.67%\n",
      "Epoch 50 Train loss: 0.196 Val loss: 0.111 Val acc: 96.67%\n",
      "Test loss: 0.108 Test acc: 96.85%\n",
      "Compression rate: 0.03125\n",
      "The number of parameters is: 24655\n",
      "Epoch 1 Train loss: 0.503 Val loss: 0.213 Val acc: 93.43%\n",
      "Epoch 2 Train loss: 0.365 Val loss: 0.185 Val acc: 94.63%\n",
      "Epoch 3 Train loss: 0.337 Val loss: 0.182 Val acc: 94.88%\n",
      "Epoch 4 Train loss: 0.319 Val loss: 0.162 Val acc: 95.18%\n",
      "Epoch 5 Train loss: 0.316 Val loss: 0.161 Val acc: 95.37%\n",
      "Epoch 6 Train loss: 0.307 Val loss: 0.152 Val acc: 95.43%\n",
      "Epoch 7 Train loss: 0.301 Val loss: 0.147 Val acc: 95.65%\n",
      "Epoch 8 Train loss: 0.295 Val loss: 0.154 Val acc: 95.45%\n",
      "Epoch 9 Train loss: 0.293 Val loss: 0.149 Val acc: 95.38%\n",
      "Epoch 10 Train loss: 0.291 Val loss: 0.141 Val acc: 95.88%\n",
      "Epoch 11 Train loss: 0.289 Val loss: 0.151 Val acc: 95.50%\n",
      "Epoch 12 Train loss: 0.279 Val loss: 0.142 Val acc: 95.85%\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 13 Train loss: 0.282 Val loss: 0.150 Val acc: 95.35%\n",
      "Epoch 14 Train loss: 0.255 Val loss: 0.128 Val acc: 96.40%\n",
      "Epoch 15 Train loss: 0.246 Val loss: 0.127 Val acc: 96.42%\n",
      "Epoch 16 Train loss: 0.244 Val loss: 0.124 Val acc: 96.50%\n",
      "Epoch 17 Train loss: 0.244 Val loss: 0.124 Val acc: 96.40%\n",
      "Epoch 18 Train loss: 0.241 Val loss: 0.122 Val acc: 96.40%\n",
      "Epoch 19 Train loss: 0.243 Val loss: 0.124 Val acc: 96.42%\n",
      "Epoch 20 Train loss: 0.238 Val loss: 0.124 Val acc: 96.58%\n",
      "Epoch 21 Train loss: 0.239 Val loss: 0.122 Val acc: 96.57%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22 Train loss: 0.239 Val loss: 0.122 Val acc: 96.45%\n",
      "Epoch 23 Train loss: 0.239 Val loss: 0.123 Val acc: 96.47%\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 24 Train loss: 0.236 Val loss: 0.122 Val acc: 96.40%\n",
      "Epoch 25 Train loss: 0.235 Val loss: 0.122 Val acc: 96.43%\n",
      "Epoch 26 Train loss: 0.237 Val loss: 0.122 Val acc: 96.50%\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 27 Train loss: 0.233 Val loss: 0.122 Val acc: 96.45%\n",
      "Epoch 28 Train loss: 0.234 Val loss: 0.122 Val acc: 96.45%\n",
      "Epoch 29 Train loss: 0.233 Val loss: 0.122 Val acc: 96.45%\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 30 Train loss: 0.233 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 31 Train loss: 0.233 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 32 Train loss: 0.238 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 33 Train loss: 0.237 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 34 Train loss: 0.232 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 35 Train loss: 0.238 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 36 Train loss: 0.231 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 37 Train loss: 0.232 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 38 Train loss: 0.238 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 39 Train loss: 0.236 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 40 Train loss: 0.236 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 41 Train loss: 0.232 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 42 Train loss: 0.238 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 43 Train loss: 0.232 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 44 Train loss: 0.237 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 45 Train loss: 0.232 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 46 Train loss: 0.235 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 47 Train loss: 0.236 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 48 Train loss: 0.233 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 49 Train loss: 0.237 Val loss: 0.122 Val acc: 96.47%\n",
      "Epoch 50 Train loss: 0.233 Val loss: 0.122 Val acc: 96.47%\n",
      "Test loss: 0.122 Test acc: 96.47%\n",
      "The number of parameters is: 24855\n",
      "Epoch 1 Train loss: 0.397 Val loss: 0.158 Val acc: 95.30%\n",
      "Epoch 2 Train loss: 0.284 Val loss: 0.135 Val acc: 96.00%\n",
      "Epoch 3 Train loss: 0.261 Val loss: 0.125 Val acc: 96.25%\n",
      "Epoch 4 Train loss: 0.261 Val loss: 0.126 Val acc: 96.20%\n",
      "Epoch 5 Train loss: 0.245 Val loss: 0.124 Val acc: 96.02%\n",
      "Epoch 6 Train loss: 0.242 Val loss: 0.116 Val acc: 96.37%\n",
      "Epoch 7 Train loss: 0.238 Val loss: 0.122 Val acc: 96.37%\n",
      "Epoch 8 Train loss: 0.232 Val loss: 0.118 Val acc: 96.62%\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 9 Train loss: 0.233 Val loss: 0.129 Val acc: 96.33%\n",
      "Epoch 10 Train loss: 0.173 Val loss: 0.100 Val acc: 97.12%\n",
      "Epoch 11 Train loss: 0.150 Val loss: 0.098 Val acc: 97.28%\n",
      "Epoch 12 Train loss: 0.148 Val loss: 0.096 Val acc: 97.12%\n",
      "Epoch 13 Train loss: 0.145 Val loss: 0.095 Val acc: 97.22%\n",
      "Epoch 14 Train loss: 0.142 Val loss: 0.092 Val acc: 97.27%\n",
      "Epoch 15 Train loss: 0.140 Val loss: 0.092 Val acc: 97.17%\n",
      "Epoch 16 Train loss: 0.137 Val loss: 0.092 Val acc: 97.25%\n",
      "Epoch 17 Train loss: 0.139 Val loss: 0.090 Val acc: 97.25%\n",
      "Epoch 18 Train loss: 0.137 Val loss: 0.089 Val acc: 97.30%\n",
      "Epoch 19 Train loss: 0.133 Val loss: 0.091 Val acc: 97.27%\n",
      "Epoch 20 Train loss: 0.130 Val loss: 0.087 Val acc: 97.28%\n",
      "Epoch 21 Train loss: 0.130 Val loss: 0.087 Val acc: 97.27%\n",
      "Epoch 22 Train loss: 0.130 Val loss: 0.089 Val acc: 97.20%\n",
      "Epoch 23 Train loss: 0.129 Val loss: 0.086 Val acc: 97.30%\n",
      "Epoch 24 Train loss: 0.131 Val loss: 0.088 Val acc: 97.27%\n",
      "Epoch 25 Train loss: 0.126 Val loss: 0.087 Val acc: 97.33%\n",
      "Epoch 26 Train loss: 0.123 Val loss: 0.086 Val acc: 97.38%\n",
      "Epoch 27 Train loss: 0.126 Val loss: 0.085 Val acc: 97.37%\n",
      "Epoch 28 Train loss: 0.127 Val loss: 0.085 Val acc: 97.35%\n",
      "Epoch 29 Train loss: 0.125 Val loss: 0.085 Val acc: 97.37%\n",
      "Epoch 30 Train loss: 0.125 Val loss: 0.084 Val acc: 97.37%\n",
      "Epoch 31 Train loss: 0.125 Val loss: 0.083 Val acc: 97.47%\n",
      "Epoch 32 Train loss: 0.121 Val loss: 0.082 Val acc: 97.55%\n",
      "Epoch 33 Train loss: 0.124 Val loss: 0.085 Val acc: 97.33%\n",
      "Epoch 34 Train loss: 0.122 Val loss: 0.084 Val acc: 97.35%\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 35 Train loss: 0.118 Val loss: 0.084 Val acc: 97.45%\n",
      "Epoch 36 Train loss: 0.118 Val loss: 0.083 Val acc: 97.40%\n",
      "Epoch 37 Train loss: 0.116 Val loss: 0.082 Val acc: 97.38%\n",
      "Epoch 38 Train loss: 0.116 Val loss: 0.081 Val acc: 97.45%\n",
      "Epoch 39 Train loss: 0.116 Val loss: 0.081 Val acc: 97.42%\n",
      "Epoch 40 Train loss: 0.117 Val loss: 0.082 Val acc: 97.40%\n",
      "Epoch 41 Train loss: 0.117 Val loss: 0.082 Val acc: 97.42%\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 42 Train loss: 0.119 Val loss: 0.081 Val acc: 97.47%\n",
      "Epoch 43 Train loss: 0.117 Val loss: 0.081 Val acc: 97.47%\n",
      "Epoch 44 Train loss: 0.118 Val loss: 0.081 Val acc: 97.47%\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 45 Train loss: 0.116 Val loss: 0.081 Val acc: 97.47%\n",
      "Epoch 46 Train loss: 0.117 Val loss: 0.081 Val acc: 97.47%\n",
      "Epoch 47 Train loss: 0.117 Val loss: 0.081 Val acc: 97.47%\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 48 Train loss: 0.115 Val loss: 0.081 Val acc: 97.47%\n",
      "Epoch 49 Train loss: 0.115 Val loss: 0.081 Val acc: 97.47%\n",
      "Epoch 50 Train loss: 0.116 Val loss: 0.081 Val acc: 97.47%\n",
      "Test loss: 0.077 Test acc: 97.75%\n",
      "Compression rate: 0.0625\n",
      "The number of parameters is: 49300\n",
      "Epoch 1 Train loss: 0.404 Val loss: 0.168 Val acc: 94.87%\n",
      "Epoch 2 Train loss: 0.256 Val loss: 0.134 Val acc: 95.82%\n",
      "Epoch 3 Train loss: 0.225 Val loss: 0.121 Val acc: 96.32%\n",
      "Epoch 4 Train loss: 0.210 Val loss: 0.115 Val acc: 96.35%\n",
      "Epoch 5 Train loss: 0.197 Val loss: 0.104 Val acc: 96.83%\n",
      "Epoch 6 Train loss: 0.192 Val loss: 0.104 Val acc: 96.87%\n",
      "Epoch 7 Train loss: 0.187 Val loss: 0.098 Val acc: 96.88%\n",
      "Epoch 8 Train loss: 0.179 Val loss: 0.100 Val acc: 96.77%\n",
      "Epoch 9 Train loss: 0.178 Val loss: 0.087 Val acc: 97.15%\n",
      "Epoch 10 Train loss: 0.173 Val loss: 0.095 Val acc: 97.00%\n",
      "Epoch 11 Train loss: 0.170 Val loss: 0.091 Val acc: 97.08%\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 12 Train loss: 0.167 Val loss: 0.090 Val acc: 97.20%\n",
      "Epoch 13 Train loss: 0.141 Val loss: 0.083 Val acc: 97.35%\n",
      "Epoch 14 Train loss: 0.140 Val loss: 0.081 Val acc: 97.47%\n",
      "Epoch 15 Train loss: 0.137 Val loss: 0.080 Val acc: 97.55%\n",
      "Epoch 16 Train loss: 0.133 Val loss: 0.080 Val acc: 97.58%\n",
      "Epoch 17 Train loss: 0.133 Val loss: 0.079 Val acc: 97.50%\n",
      "Epoch 18 Train loss: 0.135 Val loss: 0.078 Val acc: 97.57%\n",
      "Epoch 19 Train loss: 0.133 Val loss: 0.079 Val acc: 97.52%\n",
      "Epoch 20 Train loss: 0.132 Val loss: 0.078 Val acc: 97.67%\n",
      "Epoch 21 Train loss: 0.129 Val loss: 0.079 Val acc: 97.63%\n",
      "Epoch 22 Train loss: 0.129 Val loss: 0.078 Val acc: 97.53%\n",
      "Epoch 23 Train loss: 0.129 Val loss: 0.078 Val acc: 97.60%\n",
      "Epoch 24 Train loss: 0.130 Val loss: 0.078 Val acc: 97.62%\n",
      "Epoch 25 Train loss: 0.127 Val loss: 0.078 Val acc: 97.73%\n",
      "Epoch 26 Train loss: 0.129 Val loss: 0.077 Val acc: 97.63%\n",
      "Epoch 27 Train loss: 0.129 Val loss: 0.076 Val acc: 97.72%\n",
      "Epoch 28 Train loss: 0.127 Val loss: 0.077 Val acc: 97.77%\n",
      "Epoch 29 Train loss: 0.128 Val loss: 0.076 Val acc: 97.72%\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 30 Train loss: 0.127 Val loss: 0.076 Val acc: 97.77%\n",
      "Epoch 31 Train loss: 0.128 Val loss: 0.076 Val acc: 97.80%\n",
      "Epoch 32 Train loss: 0.126 Val loss: 0.076 Val acc: 97.77%\n",
      "Epoch 33 Train loss: 0.124 Val loss: 0.076 Val acc: 97.77%\n",
      "Epoch 34 Train loss: 0.121 Val loss: 0.075 Val acc: 97.77%\n",
      "Epoch 35 Train loss: 0.125 Val loss: 0.075 Val acc: 97.75%\n",
      "Epoch 36 Train loss: 0.122 Val loss: 0.075 Val acc: 97.75%\n",
      "Epoch 37 Train loss: 0.127 Val loss: 0.075 Val acc: 97.77%\n",
      "Epoch 38 Train loss: 0.123 Val loss: 0.075 Val acc: 97.75%\n",
      "Epoch 39 Train loss: 0.123 Val loss: 0.075 Val acc: 97.75%\n",
      "Epoch 40 Train loss: 0.121 Val loss: 0.075 Val acc: 97.77%\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 41 Train loss: 0.123 Val loss: 0.075 Val acc: 97.77%\n",
      "Epoch 42 Train loss: 0.126 Val loss: 0.075 Val acc: 97.75%\n",
      "Epoch 43 Train loss: 0.123 Val loss: 0.075 Val acc: 97.75%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 44 Train loss: 0.125 Val loss: 0.075 Val acc: 97.73%\n",
      "Epoch 45 Train loss: 0.123 Val loss: 0.075 Val acc: 97.73%\n",
      "Epoch 46 Train loss: 0.125 Val loss: 0.075 Val acc: 97.73%\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 47 Train loss: 0.124 Val loss: 0.075 Val acc: 97.73%\n",
      "Epoch 48 Train loss: 0.123 Val loss: 0.075 Val acc: 97.73%\n",
      "Epoch 49 Train loss: 0.123 Val loss: 0.075 Val acc: 97.73%\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 50 Train loss: 0.123 Val loss: 0.075 Val acc: 97.73%\n",
      "Test loss: 0.078 Test acc: 97.72%\n",
      "The number of parameters is: 49698\n",
      "Epoch 1 Train loss: 0.330 Val loss: 0.134 Val acc: 95.75%\n",
      "Epoch 2 Train loss: 0.205 Val loss: 0.118 Val acc: 96.47%\n",
      "Epoch 3 Train loss: 0.179 Val loss: 0.101 Val acc: 96.68%\n",
      "Epoch 4 Train loss: 0.166 Val loss: 0.097 Val acc: 97.25%\n",
      "Epoch 5 Train loss: 0.157 Val loss: 0.092 Val acc: 97.15%\n",
      "Epoch 6 Train loss: 0.151 Val loss: 0.083 Val acc: 97.60%\n",
      "Epoch 7 Train loss: 0.143 Val loss: 0.091 Val acc: 97.33%\n",
      "Epoch 8 Train loss: 0.138 Val loss: 0.089 Val acc: 97.40%\n",
      "Epoch 9 Train loss: 0.137 Val loss: 0.082 Val acc: 97.60%\n",
      "Epoch 10 Train loss: 0.135 Val loss: 0.087 Val acc: 97.33%\n",
      "Epoch 11 Train loss: 0.132 Val loss: 0.079 Val acc: 97.73%\n",
      "Epoch 12 Train loss: 0.129 Val loss: 0.082 Val acc: 97.65%\n",
      "Epoch 13 Train loss: 0.124 Val loss: 0.083 Val acc: 97.50%\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 14 Train loss: 0.127 Val loss: 0.079 Val acc: 97.68%\n",
      "Epoch 15 Train loss: 0.098 Val loss: 0.072 Val acc: 97.87%\n",
      "Epoch 16 Train loss: 0.091 Val loss: 0.068 Val acc: 97.93%\n",
      "Epoch 17 Train loss: 0.084 Val loss: 0.068 Val acc: 97.98%\n",
      "Epoch 18 Train loss: 0.080 Val loss: 0.067 Val acc: 97.97%\n",
      "Epoch 19 Train loss: 0.081 Val loss: 0.068 Val acc: 97.97%\n",
      "Epoch 20 Train loss: 0.080 Val loss: 0.067 Val acc: 97.87%\n",
      "Epoch 21 Train loss: 0.078 Val loss: 0.066 Val acc: 98.10%\n",
      "Epoch 22 Train loss: 0.077 Val loss: 0.066 Val acc: 97.97%\n",
      "Epoch 23 Train loss: 0.075 Val loss: 0.065 Val acc: 98.15%\n",
      "Epoch 24 Train loss: 0.076 Val loss: 0.065 Val acc: 98.08%\n",
      "Epoch 25 Train loss: 0.073 Val loss: 0.064 Val acc: 98.10%\n",
      "Epoch 26 Train loss: 0.074 Val loss: 0.064 Val acc: 98.10%\n",
      "Epoch 27 Train loss: 0.072 Val loss: 0.063 Val acc: 98.02%\n",
      "Epoch 28 Train loss: 0.073 Val loss: 0.062 Val acc: 98.07%\n",
      "Epoch 29 Train loss: 0.071 Val loss: 0.063 Val acc: 98.08%\n",
      "Epoch 30 Train loss: 0.071 Val loss: 0.062 Val acc: 98.13%\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 31 Train loss: 0.071 Val loss: 0.063 Val acc: 97.93%\n",
      "Epoch 32 Train loss: 0.069 Val loss: 0.062 Val acc: 98.02%\n",
      "Epoch 33 Train loss: 0.067 Val loss: 0.062 Val acc: 98.05%\n",
      "Epoch 34 Train loss: 0.070 Val loss: 0.062 Val acc: 98.03%\n",
      "Epoch 35 Train loss: 0.069 Val loss: 0.062 Val acc: 98.07%\n",
      "Epoch 36 Train loss: 0.069 Val loss: 0.062 Val acc: 98.10%\n",
      "Epoch 37 Train loss: 0.070 Val loss: 0.062 Val acc: 98.10%\n",
      "Epoch 38 Train loss: 0.068 Val loss: 0.062 Val acc: 98.12%\n",
      "Epoch 39 Train loss: 0.070 Val loss: 0.062 Val acc: 98.12%\n",
      "Epoch 40 Train loss: 0.066 Val loss: 0.062 Val acc: 98.10%\n",
      "Epoch 41 Train loss: 0.069 Val loss: 0.062 Val acc: 98.08%\n",
      "Epoch 42 Train loss: 0.070 Val loss: 0.062 Val acc: 98.10%\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 43 Train loss: 0.068 Val loss: 0.062 Val acc: 98.13%\n",
      "Epoch 44 Train loss: 0.067 Val loss: 0.062 Val acc: 98.13%\n",
      "Epoch 45 Train loss: 0.068 Val loss: 0.062 Val acc: 98.13%\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 46 Train loss: 0.067 Val loss: 0.062 Val acc: 98.13%\n",
      "Epoch 47 Train loss: 0.068 Val loss: 0.062 Val acc: 98.13%\n",
      "Epoch 48 Train loss: 0.067 Val loss: 0.062 Val acc: 98.13%\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 49 Train loss: 0.070 Val loss: 0.062 Val acc: 98.13%\n",
      "Epoch 50 Train loss: 0.066 Val loss: 0.062 Val acc: 98.13%\n",
      "Test loss: 0.065 Test acc: 98.16%\n",
      "Compression rate: 0.125\n",
      "The number of parameters is: 99385\n",
      "Epoch 1 Train loss: 0.355 Val loss: 0.145 Val acc: 95.60%\n",
      "Epoch 2 Train loss: 0.197 Val loss: 0.108 Val acc: 96.67%\n",
      "Epoch 3 Train loss: 0.165 Val loss: 0.098 Val acc: 96.72%\n",
      "Epoch 4 Train loss: 0.147 Val loss: 0.090 Val acc: 96.92%\n",
      "Epoch 5 Train loss: 0.139 Val loss: 0.084 Val acc: 97.53%\n",
      "Epoch 6 Train loss: 0.128 Val loss: 0.075 Val acc: 97.67%\n",
      "Epoch 7 Train loss: 0.122 Val loss: 0.076 Val acc: 97.67%\n",
      "Epoch 8 Train loss: 0.116 Val loss: 0.078 Val acc: 97.43%\n",
      "Epoch 9 Train loss: 0.111 Val loss: 0.074 Val acc: 97.68%\n",
      "Epoch 10 Train loss: 0.107 Val loss: 0.070 Val acc: 98.00%\n",
      "Epoch 11 Train loss: 0.104 Val loss: 0.070 Val acc: 97.77%\n",
      "Epoch 12 Train loss: 0.100 Val loss: 0.065 Val acc: 98.05%\n",
      "Epoch 13 Train loss: 0.098 Val loss: 0.071 Val acc: 97.80%\n",
      "Epoch 14 Train loss: 0.096 Val loss: 0.065 Val acc: 97.83%\n",
      "Epoch 15 Train loss: 0.094 Val loss: 0.067 Val acc: 97.88%\n",
      "Epoch 16 Train loss: 0.092 Val loss: 0.064 Val acc: 97.97%\n",
      "Epoch 17 Train loss: 0.091 Val loss: 0.067 Val acc: 97.83%\n",
      "Epoch 18 Train loss: 0.089 Val loss: 0.068 Val acc: 97.80%\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 19 Train loss: 0.090 Val loss: 0.070 Val acc: 97.77%\n",
      "Epoch 20 Train loss: 0.077 Val loss: 0.065 Val acc: 97.97%\n",
      "Epoch 21 Train loss: 0.071 Val loss: 0.063 Val acc: 98.08%\n",
      "Epoch 22 Train loss: 0.069 Val loss: 0.062 Val acc: 98.12%\n",
      "Epoch 23 Train loss: 0.066 Val loss: 0.062 Val acc: 98.13%\n",
      "Epoch 24 Train loss: 0.065 Val loss: 0.061 Val acc: 98.20%\n",
      "Epoch 25 Train loss: 0.065 Val loss: 0.060 Val acc: 98.20%\n",
      "Epoch 26 Train loss: 0.064 Val loss: 0.060 Val acc: 98.23%\n",
      "Epoch 27 Train loss: 0.064 Val loss: 0.060 Val acc: 98.23%\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 28 Train loss: 0.064 Val loss: 0.061 Val acc: 98.15%\n",
      "Epoch 29 Train loss: 0.064 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 30 Train loss: 0.063 Val loss: 0.060 Val acc: 98.13%\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 31 Train loss: 0.061 Val loss: 0.060 Val acc: 98.10%\n",
      "Epoch 32 Train loss: 0.061 Val loss: 0.060 Val acc: 98.10%\n",
      "Epoch 33 Train loss: 0.063 Val loss: 0.060 Val acc: 98.10%\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 34 Train loss: 0.061 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 35 Train loss: 0.062 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 36 Train loss: 0.062 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 37 Train loss: 0.063 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 38 Train loss: 0.063 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 39 Train loss: 0.063 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 40 Train loss: 0.062 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 41 Train loss: 0.061 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 42 Train loss: 0.063 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 43 Train loss: 0.064 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 44 Train loss: 0.061 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 45 Train loss: 0.062 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 46 Train loss: 0.060 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 47 Train loss: 0.062 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 48 Train loss: 0.062 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 49 Train loss: 0.062 Val loss: 0.060 Val acc: 98.12%\n",
      "Epoch 50 Train loss: 0.062 Val loss: 0.060 Val acc: 98.12%\n",
      "Test loss: 0.057 Test acc: 98.25%\n",
      "The number of parameters is: 99385\n",
      "Epoch 1 Train loss: 0.303 Val loss: 0.125 Val acc: 96.20%\n",
      "Epoch 2 Train loss: 0.169 Val loss: 0.097 Val acc: 97.15%\n",
      "Epoch 3 Train loss: 0.139 Val loss: 0.086 Val acc: 97.38%\n",
      "Epoch 4 Train loss: 0.123 Val loss: 0.092 Val acc: 97.22%\n",
      "Epoch 5 Train loss: 0.113 Val loss: 0.084 Val acc: 97.17%\n",
      "Epoch 6 Train loss: 0.105 Val loss: 0.083 Val acc: 97.67%\n",
      "Epoch 7 Train loss: 0.102 Val loss: 0.077 Val acc: 97.75%\n",
      "Epoch 8 Train loss: 0.095 Val loss: 0.078 Val acc: 97.63%\n",
      "Epoch 9 Train loss: 0.087 Val loss: 0.070 Val acc: 98.02%\n",
      "Epoch 10 Train loss: 0.087 Val loss: 0.068 Val acc: 97.88%\n",
      "Epoch 11 Train loss: 0.087 Val loss: 0.074 Val acc: 98.03%\n",
      "Epoch 12 Train loss: 0.080 Val loss: 0.069 Val acc: 98.02%\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 13 Train loss: 0.077 Val loss: 0.070 Val acc: 97.90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Train loss: 0.065 Val loss: 0.065 Val acc: 98.20%\n",
      "Epoch 15 Train loss: 0.055 Val loss: 0.064 Val acc: 98.27%\n",
      "Epoch 16 Train loss: 0.052 Val loss: 0.064 Val acc: 98.22%\n",
      "Epoch 17 Train loss: 0.051 Val loss: 0.063 Val acc: 98.27%\n",
      "Epoch 18 Train loss: 0.047 Val loss: 0.062 Val acc: 98.25%\n",
      "Epoch 19 Train loss: 0.047 Val loss: 0.063 Val acc: 98.25%\n",
      "Epoch 20 Train loss: 0.048 Val loss: 0.062 Val acc: 98.25%\n",
      "Epoch 21 Train loss: 0.045 Val loss: 0.062 Val acc: 98.32%\n",
      "Epoch 22 Train loss: 0.046 Val loss: 0.061 Val acc: 98.22%\n",
      "Epoch 23 Train loss: 0.045 Val loss: 0.061 Val acc: 98.32%\n",
      "Epoch 24 Train loss: 0.045 Val loss: 0.061 Val acc: 98.27%\n",
      "Epoch 25 Train loss: 0.045 Val loss: 0.060 Val acc: 98.35%\n",
      "Epoch 26 Train loss: 0.044 Val loss: 0.059 Val acc: 98.28%\n",
      "Epoch 27 Train loss: 0.042 Val loss: 0.060 Val acc: 98.22%\n",
      "Epoch 28 Train loss: 0.041 Val loss: 0.060 Val acc: 98.30%\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 29 Train loss: 0.042 Val loss: 0.061 Val acc: 98.32%\n",
      "Epoch 30 Train loss: 0.041 Val loss: 0.060 Val acc: 98.35%\n",
      "Epoch 31 Train loss: 0.041 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 32 Train loss: 0.042 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 33 Train loss: 0.042 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 34 Train loss: 0.040 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 35 Train loss: 0.041 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 36 Train loss: 0.038 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 37 Train loss: 0.041 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 38 Train loss: 0.039 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 39 Train loss: 0.041 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 40 Train loss: 0.040 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 41 Train loss: 0.041 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 42 Train loss: 0.042 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 43 Train loss: 0.042 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 44 Train loss: 0.039 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 45 Train loss: 0.041 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 46 Train loss: 0.041 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 47 Train loss: 0.041 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 48 Train loss: 0.042 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 49 Train loss: 0.040 Val loss: 0.060 Val acc: 98.33%\n",
      "Epoch 50 Train loss: 0.039 Val loss: 0.060 Val acc: 98.33%\n",
      "Test loss: 0.053 Test acc: 98.48%\n",
      "Compression rate: 1\n",
      "The number of parameters is: 795010\n",
      "Epoch 1 Train loss: 0.294 Val loss: 0.132 Val acc: 95.90%\n",
      "Epoch 2 Train loss: 0.140 Val loss: 0.094 Val acc: 97.07%\n",
      "Epoch 3 Train loss: 0.106 Val loss: 0.075 Val acc: 97.63%\n",
      "Epoch 4 Train loss: 0.087 Val loss: 0.072 Val acc: 97.95%\n",
      "Epoch 5 Train loss: 0.077 Val loss: 0.065 Val acc: 98.02%\n",
      "Epoch 6 Train loss: 0.065 Val loss: 0.067 Val acc: 98.00%\n",
      "Epoch 7 Train loss: 0.059 Val loss: 0.061 Val acc: 98.12%\n",
      "Epoch 8 Train loss: 0.055 Val loss: 0.057 Val acc: 98.12%\n",
      "Epoch 9 Train loss: 0.048 Val loss: 0.057 Val acc: 98.25%\n",
      "Epoch 10 Train loss: 0.045 Val loss: 0.059 Val acc: 98.18%\n",
      "Epoch 11 Train loss: 0.041 Val loss: 0.054 Val acc: 98.32%\n",
      "Epoch 12 Train loss: 0.037 Val loss: 0.053 Val acc: 98.40%\n",
      "Epoch 13 Train loss: 0.035 Val loss: 0.057 Val acc: 98.35%\n",
      "Epoch 14 Train loss: 0.033 Val loss: 0.057 Val acc: 98.20%\n",
      "Epoch 15 Train loss: 0.034 Val loss: 0.053 Val acc: 98.53%\n",
      "Epoch 16 Train loss: 0.030 Val loss: 0.051 Val acc: 98.38%\n",
      "Epoch 17 Train loss: 0.029 Val loss: 0.048 Val acc: 98.48%\n",
      "Epoch 18 Train loss: 0.027 Val loss: 0.052 Val acc: 98.50%\n",
      "Epoch 19 Train loss: 0.026 Val loss: 0.055 Val acc: 98.33%\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 20 Train loss: 0.025 Val loss: 0.057 Val acc: 98.45%\n",
      "Epoch 21 Train loss: 0.020 Val loss: 0.052 Val acc: 98.57%\n",
      "Epoch 22 Train loss: 0.018 Val loss: 0.050 Val acc: 98.70%\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 23 Train loss: 0.018 Val loss: 0.049 Val acc: 98.73%\n",
      "Epoch 24 Train loss: 0.016 Val loss: 0.049 Val acc: 98.75%\n",
      "Epoch 25 Train loss: 0.016 Val loss: 0.048 Val acc: 98.75%\n",
      "Epoch 26 Train loss: 0.016 Val loss: 0.048 Val acc: 98.73%\n",
      "Epoch 27 Train loss: 0.016 Val loss: 0.048 Val acc: 98.73%\n",
      "Epoch 28 Train loss: 0.016 Val loss: 0.048 Val acc: 98.73%\n",
      "Epoch 29 Train loss: 0.016 Val loss: 0.048 Val acc: 98.75%\n",
      "Epoch 30 Train loss: 0.015 Val loss: 0.048 Val acc: 98.73%\n",
      "Epoch 31 Train loss: 0.016 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 32 Train loss: 0.014 Val loss: 0.048 Val acc: 98.73%\n",
      "Epoch 33 Train loss: 0.015 Val loss: 0.048 Val acc: 98.73%\n",
      "Epoch 34 Train loss: 0.015 Val loss: 0.048 Val acc: 98.73%\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 35 Train loss: 0.015 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch 36 Train loss: 0.016 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch 37 Train loss: 0.016 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 38 Train loss: 0.016 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch 39 Train loss: 0.016 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch 40 Train loss: 0.015 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 41 Train loss: 0.017 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch 42 Train loss: 0.016 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch 43 Train loss: 0.016 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch 44 Train loss: 0.015 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch 45 Train loss: 0.017 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch 46 Train loss: 0.015 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch 47 Train loss: 0.016 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch 48 Train loss: 0.016 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch 49 Train loss: 0.016 Val loss: 0.048 Val acc: 98.72%\n",
      "Epoch 50 Train loss: 0.015 Val loss: 0.048 Val acc: 98.72%\n",
      "Test loss: 0.045 Test acc: 98.73%\n",
      "The number of parameters is: 795010\n",
      "Epoch 1 Train loss: 0.289 Val loss: 0.122 Val acc: 96.62%\n",
      "Epoch 2 Train loss: 0.142 Val loss: 0.088 Val acc: 97.23%\n",
      "Epoch 3 Train loss: 0.110 Val loss: 0.080 Val acc: 97.37%\n",
      "Epoch 4 Train loss: 0.090 Val loss: 0.075 Val acc: 97.57%\n",
      "Epoch 5 Train loss: 0.078 Val loss: 0.066 Val acc: 97.95%\n",
      "Epoch 6 Train loss: 0.069 Val loss: 0.065 Val acc: 97.87%\n",
      "Epoch 7 Train loss: 0.062 Val loss: 0.062 Val acc: 98.07%\n",
      "Epoch 8 Train loss: 0.055 Val loss: 0.060 Val acc: 98.13%\n",
      "Epoch 9 Train loss: 0.055 Val loss: 0.058 Val acc: 98.25%\n",
      "Epoch 10 Train loss: 0.048 Val loss: 0.058 Val acc: 98.45%\n",
      "Epoch 11 Train loss: 0.045 Val loss: 0.058 Val acc: 98.22%\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch 12 Train loss: 0.041 Val loss: 0.059 Val acc: 98.23%\n",
      "Epoch 13 Train loss: 0.032 Val loss: 0.053 Val acc: 98.50%\n",
      "Epoch 14 Train loss: 0.031 Val loss: 0.052 Val acc: 98.47%\n",
      "Epoch 15 Train loss: 0.027 Val loss: 0.051 Val acc: 98.55%\n",
      "Epoch 16 Train loss: 0.025 Val loss: 0.051 Val acc: 98.52%\n",
      "Epoch 17 Train loss: 0.024 Val loss: 0.050 Val acc: 98.45%\n",
      "Epoch 18 Train loss: 0.024 Val loss: 0.049 Val acc: 98.53%\n",
      "Epoch 19 Train loss: 0.023 Val loss: 0.049 Val acc: 98.58%\n",
      "Epoch 20 Train loss: 0.023 Val loss: 0.049 Val acc: 98.50%\n",
      "Epoch 21 Train loss: 0.022 Val loss: 0.050 Val acc: 98.52%\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch 22 Train loss: 0.023 Val loss: 0.049 Val acc: 98.60%\n",
      "Epoch 23 Train loss: 0.021 Val loss: 0.049 Val acc: 98.60%\n",
      "Epoch 24 Train loss: 0.022 Val loss: 0.049 Val acc: 98.60%\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch 25 Train loss: 0.021 Val loss: 0.049 Val acc: 98.60%\n",
      "Epoch 26 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 27 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch 28 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 29 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 30 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch 31 Train loss: 0.022 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 32 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33 Train loss: 0.022 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Epoch 34 Train loss: 0.022 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 35 Train loss: 0.022 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 36 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 37 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 38 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 39 Train loss: 0.022 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 40 Train loss: 0.022 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 41 Train loss: 0.022 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 42 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 43 Train loss: 0.023 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 44 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 45 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 46 Train loss: 0.022 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 47 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 48 Train loss: 0.022 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 49 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Epoch 50 Train loss: 0.021 Val loss: 0.049 Val acc: 98.62%\n",
      "Test loss: 0.048 Test acc: 98.54%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "train_loader, valid_loader, test_loader = load_data(batch_size=50, kwargs=kwargs)\n",
    "\n",
    "def train_nn(compress, hashed):\n",
    "    input_dim = 784\n",
    "    output_dim = 10\n",
    "    \n",
    "    if hashed:\n",
    "        model = HashedNet(input_dim, output_dim, 1, 1000,\n",
    "                          compress, dropout=0.25).to(device)\n",
    "    else:\n",
    "        eq_compress = get_equivalent_compression(input_dim, output_dim,\n",
    "                                                 1000, 1, compress)\n",
    "        model = Net(input_dim, output_dim, 1, 1000,\n",
    "                    eq_compress, 0.25).to(device)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01,\n",
    "                          momentum=0.9,\n",
    "                          weight_decay=0.0)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer,\n",
    "                                                     factor=0.1,\n",
    "                                                     patience=2,\n",
    "                                                     verbose=True)\n",
    "\n",
    "    print('The number of parameters is: {}'.format(\n",
    "        sum(p.numel() for p in model.parameters() if p.requires_grad)))\n",
    "\n",
    "    for epoch in range(1, 50 + 1):\n",
    "        tr_loss = train(model, device, train_loader, optimizer, epoch, log_interval=50)\n",
    "        val_loss, val_acc = evaluate(model, device, valid_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        print('Epoch {} Train loss: {:.3f} Val loss: {:.3f} Val acc: {:.2f}%'.format(\n",
    "              epoch, tr_loss, val_loss, val_acc))\n",
    "\n",
    "    test_loss, test_acc = evaluate(model, device, test_loader)\n",
    "    print('Test loss: {:.3f} Test acc: {:.2f}%'.format(test_loss, test_acc))\n",
    "    \n",
    "    return test_loss, test_acc\n",
    "\n",
    "compression_rates = [1/64, 1/32, 1/16, 1/8, 1]\n",
    "nn_records = []\n",
    "hashednn_records = []\n",
    "for compression in compression_rates:\n",
    "    print(\"Compression rate: {}\".format(compression))\n",
    "    nn_records.append(train_nn(compression, hashed=False))\n",
    "    hashednn_records.append(train_nn(compression, hashed=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6,4))\n",
    "ax.plot([100.0-x[1] for x in hashednn_records], 's-', linewidth=2, label='Hashed NN')\n",
    "ax.plot([100.0-x[1] for x in nn_records], 'd--', linewidth=2, label='Equiv. NN')\n",
    "ax.set_xticklabels(['1/64', '1/32', '1/16', '1/8', '1'])\n",
    "ax.set_xticks(range(len(compression_rates)))\n",
    "ax.set_xlabel('Compression ratio')\n",
    "ax.set_ylabel('Error (%)')\n",
    "plt.title(\"1 hidden layer, 1000 units\")\n",
    "plt.legend(handlelength=3)\n",
    "plt.grid()\n",
    "plt.savefig('example.svg')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch3.7] *",
   "language": "python",
   "name": "conda-env-pytorch3.7-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
